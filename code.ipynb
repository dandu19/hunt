{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1/fzLFW+h3ZHiAblKBiyw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dandu19/hunt/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JRaY2cgE1wk"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Cognizant AI\n",
        "# ## Stock Prediction (ML Production)\n",
        "#\n",
        "#\n",
        "# **Purpose:** This assessment is an extensive machine learning project. The task is to prepare a Python module that contains code to train a model and output the performance metrics when the file is run, assuming that the Python file does not need to process, clean or transform the dataset.\n",
        "#\n",
        "\n",
        "# ##  1. Importing important packages and libraries\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import pandas as pd\n",
        "import statistics\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "# ## 2. Data loading\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# Loading & printing the dataset\n",
        "path = \"D:/AI or Data Science Internships/Cognizant AI/Task-3/cognizantai_merged_dataset.csv\"\n",
        "\n",
        "sp_df = pd.read_csv(path)\n",
        "sp_df.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')\n",
        "sp_df.head()\n",
        "\n",
        "\n",
        "# ## 3. Modelling & Evaluation\n",
        "#\n",
        "# Now, we will train the machine learning model. We will use a supervised machine learning model, and we will use `estimated_stock_pct` as the target variable, since the problem statement was focused on being able to predict the stock levels of products on an hourly basis.\n",
        "#\n",
        "# Whilst training the machine learning model, we will use `cross-validation`, which is a technique where we hold back a portion of the dataset for testing in order to compute how well the trained machine learning model is able to predict the target variable.\n",
        "#\n",
        "# Finally, to ensure that the trained machine learning model is able to perform robustly, we will want to test it several times on random samples of data, not just once. Hence, we will use a `K-fold` strategy to train the machine learning model on `K` (K is an integer to be decided) random samples of the data.\n",
        "#\n",
        "# First, let's create our target variable `y` and independent variables `X`\n",
        "\n",
        "# ### Splitting the dataset into Training and Test data\n",
        "\n",
        "# In[3]:\n",
        "\n",
        "\n",
        "# Splitting data into features & target variable\n",
        "X = sp_df.drop(columns=['estimated_stock_pct'])\n",
        "y = sp_df['estimated_stock_pct']\n",
        "\n",
        "# Printing the dimensions of features & target variable\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# This shows that we have 28 predictor variables that we will train our machine learning model on and 10845 rows of data.\n",
        "#\n",
        "# Now let's define how many folds we want to complete during training, and how much of the dataset to assign to training (75%), leaving the rest for test (25%).\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "# Defining number of folds for cross validation & splitting the dataset\n",
        "K = 10\n",
        "split = 0.75\n",
        "\n",
        "\n",
        "# we are going to use a `RandomForestRegressor` model, which is an instance of a Random Forest.\n",
        "#\n",
        "# We are using a `regression` algorithm here because we are predicting a continuous numeric variable, that is, `estimated_stock_pct`. A `classification` algorithm would be suitable for scenarios where you're predicted a binary outcome, e.g. True/False.\n",
        "\n",
        "# And now let's create a loop to train `K` models with a 75/25% random split of the data each time between training and test samples\n",
        "\n",
        "# ### Building the Random Forest Regressor & evaluating Mean Absolute Error (MAE)\n",
        "\n",
        "# In[5]:\n",
        "\n",
        "\n",
        "# Create a list that will store the accuracies of each fold\n",
        "accuracy = []\n",
        "\n",
        "# Enter a loop to run K folds of cross-validation\n",
        "for fold in range(0, K):\n",
        "\n",
        "  # Instantiate algorithm\n",
        "  model = RandomForestRegressor()\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  # Create training and test samples\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=split, random_state=42)\n",
        "\n",
        "  # Scale X data, we scale the data because it helps the algorithm to converge\n",
        "  # and helps the algorithm to not be greedy with large values\n",
        "  scaler.fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  # Train model\n",
        "  trained_model = model.fit(X_train, y_train)\n",
        "\n",
        "  # Generate predictions on test sample\n",
        "  y_pred = trained_model.predict(X_test)\n",
        "\n",
        "  # Compute accuracy, using mean absolute error\n",
        "  mae = mean_absolute_error(y_true=y_test, y_pred=y_pred)\n",
        "  accuracy.append(mae)\n",
        "  print(f\"Fold {fold + 1}: MAE = {mae:.3f}\")\n",
        "\n",
        "# Finish by computing the average MAE across all folds\n",
        "print(f\"Average MAE: {(sum(accuracy) / len(accuracy)):.2f}\")\n",
        "\n",
        "\n",
        "# The `mean absolute error` (MAE) is almost exactly the same each time which shows that the performance of the model is consistent across different random samples of the data, which is what required by us & shows a robust nature of the model.\n",
        "#\n",
        "# The `MAE` was chosen as a performance metric because it describes how closely the machine learning model was able to predict the exact value of `estimated_stock_pct`.\n",
        "#\n",
        "# Even though the model is predicting robustly, this value for MAE is unimportant, since the average value of the target variable (accuracy as a percentage) is around 0.24 (24%) which is very large. In an ideal world, we would want the MAE to be as low as possible. Through iterative process of machine learning comes using more data we can gain better accuracy. At this stage, since we only have small samples of the data, we can report back to the business with these findings and recommend that the dataset needs to be further engineered, or more datasets need to be added.\n",
        "#\n",
        "# Furthermore, we can use the trained model to intepret which features were signficant when the model was predicting the target variable.\n",
        "\n",
        "# ### Feature selection\n",
        "\n",
        "# In[6]:\n",
        "\n",
        "\n",
        "def print_best_worst (scores):\n",
        "    scores = sorted(scores, reverse = True)\n",
        "\n",
        "    print(\"The 5 best features selected by this method are :\")\n",
        "    for i in range(5):\n",
        "        print(scores[i][1])\n",
        "\n",
        "    print (\"The 5 worst features selected by this method are :\")\n",
        "    for i in range(5):\n",
        "        print(scores[len(scores)-1-i][1])\n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "\n",
        "# define the model\n",
        "rfmodel_feat = RandomForestRegressor()\n",
        "\n",
        "# fit the model\n",
        "rfmodel_feat.fit(X, y)\n",
        "\n",
        "# get importance\n",
        "rfimportance = rfmodel_feat.feature_importances_\n",
        "\n",
        "# summarize feature importance\n",
        "for i,v in enumerate(rfimportance):\n",
        "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "\n",
        "scores = []\n",
        "num_features = len(X.columns)\n",
        "\n",
        "for i in range(num_features):\n",
        "    scores.append((rfmodel_feat.feature_importances_[i],X.columns[i]))\n",
        "\n",
        "print_best_worst(scores)\n",
        "\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "# Performing Feature importance to check relevant features in the dataset\n",
        "features = [i.split(\"__\")[0] for i in X.columns]\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 20))\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.savefig('feature_importances.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# **Through feature importance we can conclude that:**\n",
        "#\n",
        "# - The product categories were insignificant\n",
        "# - The unit price and temperature were crucial in predicting stock\n",
        "# - The hour of day was also important for predicting stock"
      ]
    }
  ]
}